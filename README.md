# Sistema de Fine-tuning PTT5 com QLoRA

Um sistema completo para fine-tuning do modelo PTT5 (Portuguese T5) usando quantizaÃ§Ã£o 4-bit e adaptadores LoRA, otimizado para geraÃ§Ã£o de texto personalizada.

## ğŸ¯ VisÃ£o Geral

Este projeto implementa um pipeline completo de fine-tuning para o modelo PTT5 base, utilizando tÃ©cnicas avanÃ§adas de otimizaÃ§Ã£o como:

- **QuantizaÃ§Ã£o 4-bit**: Reduz o uso de memÃ³ria GPU em ~75%
- **QLoRA**: CombinaÃ§Ã£o de quantizaÃ§Ã£o com adaptadores LoRA
- **PEFT (Parameter-Efficient Fine-Tuning)**: Treina apenas uma pequena fraÃ§Ã£o dos parÃ¢metros
- **MÃ©tricas robustas**: AvaliaÃ§Ã£o com ROUGE e BLEU

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos
- Python 3.8+
- CUDA 11.8+ (para GPU)
- 8GB+ de RAM
- 4GB+ de VRAM (recomendado)

### InstalaÃ§Ã£o
```bash
# Clone o repositÃ³rio
git clone https://github.com/renatobarros-ai/sistema_le_mat.git
cd sistema_le_mat

# Instale as dependÃªncias
pip install -r requirements.txt
```

### Uso BÃ¡sico
```bash
# Execute o treinamento
python train_model.py
```

## ğŸ“Š Estrutura do Projeto

```
sistema_le_mat/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ training_config.yaml    # ConfiguraÃ§Ãµes de treinamento
â”œâ”€â”€ database/
â”‚   â””â”€â”€ db_462.xlsx            # Dataset (nÃ£o versionado)
â”œâ”€â”€ docs/                      # DocumentaÃ§Ã£o
â”œâ”€â”€ model_save/                # Modelos treinados
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ pessoa_x_prompt.py     # Templates de prompt
â”œâ”€â”€ results/                   # Logs e mÃ©tricas
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_processing.py     # Processamento de dados
â”‚   â””â”€â”€ evaluation_metrics.py  # MÃ©tricas de avaliaÃ§Ã£o
â”œâ”€â”€ train_model.py             # Script principal
â””â”€â”€ requirements.txt           # DependÃªncias
```

## ğŸ“š DocumentaÃ§Ã£o Completa

### ğŸ”§ ConfiguraÃ§Ã£o e InstalaÃ§Ã£o
- **[Guia de InstalaÃ§Ã£o](docs/installation.md)** - InstalaÃ§Ã£o detalhada e configuraÃ§Ã£o do ambiente
- **[ConfiguraÃ§Ã£o do Sistema](docs/configuration.md)** - PersonalizaÃ§Ã£o de parÃ¢metros e configuraÃ§Ãµes

### ğŸ—ï¸ Arquitetura e Desenvolvimento
- **[Arquitetura do Sistema](docs/architecture.md)** - VisÃ£o tÃ©cnica detalhada da implementaÃ§Ã£o
- **[Processamento de Dados](docs/data-processing.md)** - Como os dados sÃ£o processados e preparados

### ğŸ“– Guias de Uso
- **[Guia de Uso](docs/usage.md)** - Como usar o sistema e interpretar resultados
- **[Troubleshooting](docs/troubleshooting.md)** - SoluÃ§Ã£o de problemas comuns

### ğŸ”’ Dados e Privacidade
- **[Datasets e LGPD](docs/datasets.md)** - InformaÃ§Ãµes sobre dados e conformidade

## ğŸ¯ CaracterÃ­sticas Principais

### âš¡ Otimizado para EficiÃªncia
- **QuantizaÃ§Ã£o 4-bit**: Reduz drasticamente o uso de memÃ³ria
- **LoRA**: Treina apenas 0.1% dos parÃ¢metros do modelo
- **Gradient Checkpointing**: Economia adicional de memÃ³ria

### ğŸ¨ FlexÃ­vel e ConfigurÃ¡vel
- **ConfiguraÃ§Ã£o YAML**: FÃ¡cil personalizaÃ§Ã£o de parÃ¢metros
- **Templates de Prompt**: Sistema modular para diferentes estilos
- **MÃ©tricas Robustas**: AvaliaÃ§Ã£o confiÃ¡vel com ROUGE e BLEU

### ğŸ”§ Pronto para ProduÃ§Ã£o
- **Logging Completo**: Rastreamento detalhado do treinamento
- **Tratamento de Erros**: Sistema robusto de recuperaÃ§Ã£o
- **Salvamento AutomÃ¡tico**: Checkpoints e modelos preservados

## ğŸ“ˆ Requisitos de Hardware

| Componente | MÃ­nimo | Recomendado |
|------------|---------|-------------|
| **GPU** | 4GB VRAM | 8GB+ VRAM |
| **RAM** | 8GB | 16GB+ |
| **Armazenamento** | 10GB | 20GB+ |
| **CUDA** | 11.8+ | 12.0+ |

## ğŸ”„ Fluxo de Trabalho

1. **PreparaÃ§Ã£o**: Configurar ambiente e dataset
2. **ConfiguraÃ§Ã£o**: Ajustar parÃ¢metros no arquivo YAML
3. **Treinamento**: Executar o script principal
4. **AvaliaÃ§Ã£o**: Analisar mÃ©tricas e resultados
5. **InferÃªncia**: Usar o modelo treinado

## ğŸ“Š MÃ©tricas de AvaliaÃ§Ã£o

O sistema utiliza mÃ©tricas padrÃ£o da Ã¡rea:
- **ROUGE-L**: Medida de qualidade baseada em subsequÃªncias
- **BLEU**: MÃ©trica de similaridade com referÃªncia
- **Loss**: Perda de treinamento e validaÃ§Ã£o

## ğŸ¤ ContribuiÃ§Ã£o

Para contribuir com o projeto:
1. FaÃ§a um fork do repositÃ³rio
2. Crie uma branch para sua feature
3. Implemente as mudanÃ§as
4. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a [LicenÃ§a MIT](LICENSE) - veja o arquivo LICENSE para mais detalhes.

## ğŸ†˜ Suporte

- **Issues**: Use o sistema de issues do GitHub
- **DocumentaÃ§Ã£o**: Consulte a pasta [docs/](docs/) para informaÃ§Ãµes detalhadas
- **Troubleshooting**: Veja o [guia de soluÃ§Ã£o de problemas](docs/troubleshooting.md)
- **Contato direto**: falecomrenatobarros@gmail.com

---

**Desenvolvido por Renato Barros** - Sistema de Fine-tuning PTT5 com QLoRA

ğŸ“§ **Contato:** falecomrenatobarros@gmail.com
