# Processamento de Dados

Este documento detalha como o sistema processa e prepara os dados para o fine-tuning do modelo PTT5.

## Pipeline de Processamento

### 1. Carregamento dos Dados

```python
# Carregamento do arquivo Excel
df = pd.read_excel(dataset_path)
logger.info(f"Dados carregados: {len(df)} registros")
```

**Formato suportado:**
- Arquivo Excel (.xlsx)
- Encoding UTF-8
- Estrutura tabular com colunas espec√≠ficas

### 2. Valida√ß√£o e Limpeza

```python
# Valida√ß√£o das colunas obrigat√≥rias
required_columns = ['texto', 'carta', 'evento', 'secao', 'tema']
df = validate_dataframe(df, required_columns)
```

**Valida√ß√µes aplicadas:**
- Verifica√ß√£o de colunas obrigat√≥rias
- Remo√ß√£o de registros com valores nulos
- Limpeza de espa√ßos em branco
- Normaliza√ß√£o de texto

### 3. Divis√£o Estratificada

```python
# Divis√£o mantendo propor√ß√£o de cartas
train_df, val_df = train_test_split(
    df, 
    test_size=0.15, 
    random_state=42, 
    stratify=df['carta']
)
```

**Vantagens da estratifica√ß√£o:**
- Mant√©m distribui√ß√£o equilibrada
- Garante representatividade
- Evita vi√©s em conjuntos pequenos

## Fun√ß√£o de Valida√ß√£o

### Implementa√ß√£o Completa

```python
def validate_dataframe(df, required_columns):
    """
    Valida a estrutura do DataFrame e remove registros com valores nulos
    
    Args:
        df (pd.DataFrame): DataFrame a ser validado
        required_columns (list): Lista de colunas obrigat√≥rias
        
    Returns:
        pd.DataFrame: DataFrame validado e limpo
        
    Raises:
        ValueError: Se alguma coluna obrigat√≥ria estiver ausente
    """
    # Verifica√ß√£o da presen√ßa das colunas obrigat√≥rias
    if not all(col in df.columns for col in required_columns):
        raise ValueError(f"DataFrame faltando colunas necess√°rias: {required_columns}")
    
    # Registro do n√∫mero original de linhas
    original_rows = len(df)
    
    # Remo√ß√£o de linhas com valores nulos nas colunas essenciais
    df = df.dropna(subset=required_columns)
    if len(df) < original_rows:
        print(f"ATEN√á√ÉO: {original_rows - len(df)} linhas removidas devido a valores nulos em colunas essenciais.")
    
    # Limpeza e valida√ß√£o do conte√∫do de texto
    df['texto'] = df['texto'].apply(clean_text)
    df = df.dropna(subset=['texto'])
    
    # Verifica√ß√£o adicional ap√≥s limpeza de texto
    if len(df) < original_rows:
        print(f"ATEN√á√ÉO: Mais {original_rows - len(df)} linhas removidas ap√≥s limpeza de texto vazio.")

    return df
```

### Limpeza de Texto

```python
def clean_text(text):
    """
    Limpa e normaliza texto
    
    Args:
        text: Texto original
    
    Returns:
        Texto limpo ou None se inv√°lido
    """
    if pd.isna(text):
        return None
    
    # Convers√£o para string
    text = str(text)
    
    # Remo√ß√£o de espa√ßos m√∫ltiplos
    text = re.sub(r'\s+', ' ', text)
    
    # Remo√ß√£o de espa√ßos nas extremidades
    text = text.strip()
    
    # Verifica√ß√£o de texto vazio
    if not text:
        return None
    
    return text
```

## Prepara√ß√£o para Fine-tuning

### Aplica√ß√£o de Templates

```python
def prepare_data_for_fine_tuning(dataframe, prompt_template):
    """
    Prepara dados aplicando template de prompt
    
    Args:
        dataframe: DataFrame com dados limpos
        prompt_template: Template com placeholders
    
    Returns:
        Lista de pares input-output
    """
    data = []
    
    for _, row in dataframe.iterrows():
        # Formata√ß√£o do prompt
        prompt = prompt_template.format(
            carta=row['carta'],
            evento=row['evento'],
            secao=row['secao'],
            tema=row['tema']
        )
        
        # Cria√ß√£o do par treino
        data.append({
            "input": prompt,
            "output": row['texto']
        })
    
    return data
```

### Exemplo de Transforma√ß√£o

**Entrada:**
```
carta: "O Mago"
evento: "Ano Novo"
secao: "Geral"
tema: "Trabalho"
texto: "2025 √© um ano para focar..."
```

**Sa√≠da:**
```
input: "Gere uma interpreta√ß√£o para a carta 'O Mago' em 'Ano Novo', na se√ß√£o 'Geral', sobre o tema 'Trabalho', com o estilo da pessoa X."
output: "2025 √© um ano para focar..."
```

## üî§ Tokeniza√ß√£o

### Processo de Tokeniza√ß√£o

```python
def tokenize_function(examples, tokenizer, max_length):
    """
    Tokeniza exemplos para treinamento
    
    Args:
        examples: Batch de exemplos
        tokenizer: Tokenizador do modelo
        max_length: Comprimento m√°ximo
    
    Returns:
        Dicion√°rio com tokens e labels
    """
    # Tokeniza√ß√£o dos inputs
    model_inputs = tokenizer(
        examples["input"],
        max_length=max_length,
        truncation=True,
        padding=False
    )
    
    # Tokeniza√ß√£o dos outputs
    labels = tokenizer(
        text_target=examples["output"],
        max_length=max_length,
        truncation=True,
        padding=False
    )
    
    # Adi√ß√£o dos labels
    model_inputs["labels"] = labels["input_ids"]
    
    return model_inputs
```

### Configura√ß√µes de Tokeniza√ß√£o

```python
# Par√¢metros da tokeniza√ß√£o
tokenizer_config = {
    "max_length": 450,          # Comprimento m√°ximo
    "truncation": True,         # Truncar se necess√°rio
    "padding": False,           # Padding no DataCollator
    "add_special_tokens": True  # Tokens especiais
}
```

## Estat√≠sticas dos Dados

### M√©tricas Calculadas

```python
def calculate_data_statistics(df):
    """
    Calcula estat√≠sticas dos dados
    
    Args:
        df: DataFrame processado
    
    Returns:
        Dicion√°rio com estat√≠sticas
    """
    stats = {
        'total_samples': len(df),
        'unique_cartas': df['carta'].nunique(),
        'unique_eventos': df['evento'].nunique(),
        'unique_secoes': df['secao'].nunique(),
        'unique_temas': df['tema'].nunique(),
        'avg_text_length': df['texto'].str.len().mean(),
        'min_text_length': df['texto'].str.len().min(),
        'max_text_length': df['texto'].str.len().max(),
        'median_text_length': df['texto'].str.len().median()
    }
    
    return stats
```

### Distribui√ß√£o dos Dados

```python
def analyze_data_distribution(df):
    """
    Analisa distribui√ß√£o dos dados
    """
    print("=== DISTRIBUI√á√ÉO DOS DADOS ===")
    
    # Distribui√ß√£o por carta
    print("\nDistribui√ß√£o por carta:")
    carta_counts = df['carta'].value_counts()
    print(carta_counts)
    
    # Distribui√ß√£o por evento
    print("\nDistribui√ß√£o por evento:")
    evento_counts = df['evento'].value_counts()
    print(evento_counts)
    
    # Estat√≠sticas de comprimento
    print("\nEstat√≠sticas de comprimento:")
    length_stats = df['texto'].str.len().describe()
    print(length_stats)
```

## Valida√ß√£o de Qualidade

### Verifica√ß√µes Autom√°ticas

```python
def validate_data_quality(df):
    """
    Valida qualidade dos dados
    
    Args:
        df: DataFrame a ser validado
    
    Returns:
        Relat√≥rio de qualidade
    """
    issues = []
    
    # Verificar textos muito curtos
    short_texts = df[df['texto'].str.len() < 50]
    if len(short_texts) > 0:
        issues.append(f"Textos curtos (<50 chars): {len(short_texts)}")
    
    # Verificar textos muito longos
    long_texts = df[df['texto'].str.len() > 1000]
    if len(long_texts) > 0:
        issues.append(f"Textos longos (>1000 chars): {len(long_texts)}")
    
    # Verificar distribui√ß√£o de cartas
    min_samples_per_carta = df['carta'].value_counts().min()
    if min_samples_per_carta < 5:
        issues.append(f"Cartas com poucos exemplos: {min_samples_per_carta}")
    
    # Verificar duplicatas
    duplicates = df.duplicated(subset=['carta', 'evento', 'secao', 'tema']).sum()
    if duplicates > 0:
        issues.append(f"Registros duplicados: {duplicates}")
    
    return issues
```

## Visualiza√ß√£o dos Dados

### An√°lise Estat√≠stica

Para visualizar a distribui√ß√£o dos dados, voc√™ pode usar bibliotecas como matplotlib e seaborn. O sistema n√£o inclui fun√ß√µes de visualiza√ß√£o pr√©-implementadas, mas as estat√≠sticas b√°sicas s√£o calculadas automaticamente durante o processamento.

## Otimiza√ß√µes de Processamento

### Processamento Eficiente

O sistema processa os dados de forma eficiente usando:

- **Pandas**: Para manipula√ß√£o r√°pida de DataFrames
- **Processamento em lote**: Durante a tokeniza√ß√£o (configurado no DataLoader)
- **Valida√ß√£o incremental**: Verifica√ß√µes aplicadas conforme necess√°rio

Para otimiza√ß√µes adicionais, voc√™ pode implementar:
- Cache de dados processados usando pickle ou parquet
- Processamento paralelo para datasets muito grandes
- Valida√ß√£o ass√≠ncrona para melhor performance

## Pr√≥ximos Passos

Ap√≥s o processamento dos dados:

1. **Validar** qualidade dos dados processados
2. **Analisar** distribui√ß√µes e estat√≠sticas
3. **Configurar** par√¢metros de treinamento
4. **Executar** tokeniza√ß√£o
5. **Iniciar** fine-tuning

---

**Pr√≥ximo**: [Arquitetura do Sistema](architecture.md)