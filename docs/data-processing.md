# üîÑ Processamento de Dados

Este documento detalha como o sistema processa e prepara os dados para o fine-tuning do modelo PTT5.

## üìä Pipeline de Processamento

### 1. Carregamento dos Dados

```python
# Carregamento do arquivo Excel
df = pd.read_excel(dataset_path)
logger.info(f"Dados carregados: {len(df)} registros")
```

**Formato suportado:**
- Arquivo Excel (.xlsx)
- Encoding UTF-8
- Estrutura tabular com colunas espec√≠ficas

### 2. Valida√ß√£o e Limpeza

```python
# Valida√ß√£o das colunas obrigat√≥rias
required_columns = ['texto', 'carta', 'evento', 'secao', 'tema']
df = validate_dataframe(df, required_columns)
```

**Valida√ß√µes aplicadas:**
- Verifica√ß√£o de colunas obrigat√≥rias
- Remo√ß√£o de registros com valores nulos
- Limpeza de espa√ßos em branco
- Normaliza√ß√£o de texto

### 3. Divis√£o Estratificada

```python
# Divis√£o mantendo propor√ß√£o de cartas
train_df, val_df = train_test_split(
    df, 
    test_size=0.15, 
    random_state=42, 
    stratify=df['carta']
)
```

**Vantagens da estratifica√ß√£o:**
- Mant√©m distribui√ß√£o equilibrada
- Garante representatividade
- Evita vi√©s em conjuntos pequenos

## üõ†Ô∏è Fun√ß√£o de Valida√ß√£o

### Implementa√ß√£o Completa

```python
def validate_dataframe(df, required_columns):
    """
    Valida e limpa o DataFrame de entrada
    
    Args:
        df: DataFrame original
        required_columns: Lista de colunas obrigat√≥rias
    
    Returns:
        DataFrame validado e limpo
    """
    # Verifica√ß√£o de colunas
    missing_cols = [col for col in required_columns if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Colunas faltando: {missing_cols}")
    
    original_rows = len(df)
    
    # Remo√ß√£o de nulos
    df = df.dropna(subset=required_columns)
    
    # Limpeza de texto
    df['texto'] = df['texto'].apply(clean_text)
    df = df.dropna(subset=['texto'])
    
    # Relat√≥rio de limpeza
    cleaned_rows = len(df)
    if cleaned_rows < original_rows:
        print(f"Registros removidos: {original_rows - cleaned_rows}")
    
    return df
```

### Limpeza de Texto

```python
def clean_text(text):
    """
    Limpa e normaliza texto
    
    Args:
        text: Texto original
    
    Returns:
        Texto limpo ou None se inv√°lido
    """
    if pd.isna(text):
        return None
    
    # Convers√£o para string
    text = str(text)
    
    # Remo√ß√£o de espa√ßos m√∫ltiplos
    text = re.sub(r'\s+', ' ', text)
    
    # Remo√ß√£o de espa√ßos nas extremidades
    text = text.strip()
    
    # Verifica√ß√£o de texto vazio
    if not text:
        return None
    
    return text
```

## üéØ Prepara√ß√£o para Fine-tuning

### Aplica√ß√£o de Templates

```python
def prepare_data_for_fine_tuning(dataframe, prompt_template):
    """
    Prepara dados aplicando template de prompt
    
    Args:
        dataframe: DataFrame com dados limpos
        prompt_template: Template com placeholders
    
    Returns:
        Lista de pares input-output
    """
    data = []
    
    for _, row in dataframe.iterrows():
        # Formata√ß√£o do prompt
        prompt = prompt_template.format(
            carta=row['carta'],
            evento=row['evento'],
            secao=row['secao'],
            tema=row['tema']
        )
        
        # Cria√ß√£o do par treino
        data.append({
            "input": prompt,
            "output": row['texto']
        })
    
    return data
```

### Exemplo de Transforma√ß√£o

**Entrada:**
```
carta: "O Mago"
evento: "Ano Novo"
secao: "Geral"
tema: "Trabalho"
texto: "2025 √© um ano para focar..."
```

**Sa√≠da:**
```
input: "Gere uma interpreta√ß√£o para a carta 'O Mago' em 'Ano Novo', na se√ß√£o 'Geral', sobre o tema 'Trabalho', com o estilo da pessoa X."
output: "2025 √© um ano para focar..."
```

## üî§ Tokeniza√ß√£o

### Processo de Tokeniza√ß√£o

```python
def tokenize_function(examples, tokenizer, max_length):
    """
    Tokeniza exemplos para treinamento
    
    Args:
        examples: Batch de exemplos
        tokenizer: Tokenizador do modelo
        max_length: Comprimento m√°ximo
    
    Returns:
        Dicion√°rio com tokens e labels
    """
    # Tokeniza√ß√£o dos inputs
    model_inputs = tokenizer(
        examples["input"],
        max_length=max_length,
        truncation=True,
        padding=False
    )
    
    # Tokeniza√ß√£o dos outputs
    labels = tokenizer(
        text_target=examples["output"],
        max_length=max_length,
        truncation=True,
        padding=False
    )
    
    # Adi√ß√£o dos labels
    model_inputs["labels"] = labels["input_ids"]
    
    return model_inputs
```

### Configura√ß√µes de Tokeniza√ß√£o

```python
# Par√¢metros da tokeniza√ß√£o
tokenizer_config = {
    "max_length": 450,          # Comprimento m√°ximo
    "truncation": True,         # Truncar se necess√°rio
    "padding": False,           # Padding no DataCollator
    "add_special_tokens": True  # Tokens especiais
}
```

## üìà Estat√≠sticas dos Dados

### M√©tricas Calculadas

```python
def calculate_data_statistics(df):
    """
    Calcula estat√≠sticas dos dados
    
    Args:
        df: DataFrame processado
    
    Returns:
        Dicion√°rio com estat√≠sticas
    """
    stats = {
        'total_samples': len(df),
        'unique_cartas': df['carta'].nunique(),
        'unique_eventos': df['evento'].nunique(),
        'unique_secoes': df['secao'].nunique(),
        'unique_temas': df['tema'].nunique(),
        'avg_text_length': df['texto'].str.len().mean(),
        'min_text_length': df['texto'].str.len().min(),
        'max_text_length': df['texto'].str.len().max(),
        'median_text_length': df['texto'].str.len().median()
    }
    
    return stats
```

### Distribui√ß√£o dos Dados

```python
def analyze_data_distribution(df):
    """
    Analisa distribui√ß√£o dos dados
    """
    print("=== DISTRIBUI√á√ÉO DOS DADOS ===")
    
    # Distribui√ß√£o por carta
    print("\nDistribui√ß√£o por carta:")
    carta_counts = df['carta'].value_counts()
    print(carta_counts)
    
    # Distribui√ß√£o por evento
    print("\nDistribui√ß√£o por evento:")
    evento_counts = df['evento'].value_counts()
    print(evento_counts)
    
    # Estat√≠sticas de comprimento
    print("\nEstat√≠sticas de comprimento:")
    length_stats = df['texto'].str.len().describe()
    print(length_stats)
```

## üîç Valida√ß√£o de Qualidade

### Verifica√ß√µes Autom√°ticas

```python
def validate_data_quality(df):
    """
    Valida qualidade dos dados
    
    Args:
        df: DataFrame a ser validado
    
    Returns:
        Relat√≥rio de qualidade
    """
    issues = []
    
    # Verificar textos muito curtos
    short_texts = df[df['texto'].str.len() < 50]
    if len(short_texts) > 0:
        issues.append(f"Textos curtos (<50 chars): {len(short_texts)}")
    
    # Verificar textos muito longos
    long_texts = df[df['texto'].str.len() > 1000]
    if len(long_texts) > 0:
        issues.append(f"Textos longos (>1000 chars): {len(long_texts)}")
    
    # Verificar distribui√ß√£o de cartas
    min_samples_per_carta = df['carta'].value_counts().min()
    if min_samples_per_carta < 5:
        issues.append(f"Cartas com poucos exemplos: {min_samples_per_carta}")
    
    # Verificar duplicatas
    duplicates = df.duplicated(subset=['carta', 'evento', 'secao', 'tema']).sum()
    if duplicates > 0:
        issues.append(f"Registros duplicados: {duplicates}")
    
    return issues
```

## üìä Visualiza√ß√£o dos Dados

### Gr√°ficos de Distribui√ß√£o

```python
def plot_data_distribution(df):
    """
    Gera gr√°ficos de distribui√ß√£o
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Distribui√ß√£o por carta
    df['carta'].value_counts().plot(kind='bar', ax=axes[0,0])
    axes[0,0].set_title('Distribui√ß√£o por Carta')
    
    # Distribui√ß√£o por evento
    df['evento'].value_counts().plot(kind='bar', ax=axes[0,1])
    axes[0,1].set_title('Distribui√ß√£o por Evento')
    
    # Distribui√ß√£o de comprimento
    df['texto'].str.len().hist(bins=30, ax=axes[1,0])
    axes[1,0].set_title('Distribui√ß√£o de Comprimento')
    
    # Boxplot de comprimento por carta
    sns.boxplot(data=df, x='carta', y=df['texto'].str.len(), ax=axes[1,1])
    axes[1,1].set_title('Comprimento por Carta')
    axes[1,1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('data_distribution.png')
    plt.close()
```

## üéØ Otimiza√ß√µes de Processamento

### Processamento em Lote

```python
def process_in_batches(df, batch_size=1000):
    """
    Processa dados em lotes para efici√™ncia
    
    Args:
        df: DataFrame a ser processado
        batch_size: Tamanho do lote
    
    Yields:
        Lotes processados
    """
    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]
        yield process_batch(batch)
```

### Cache de Processamento

```python
def cache_processed_data(df, cache_path):
    """
    Salva dados processados em cache
    
    Args:
        df: DataFrame processado
        cache_path: Caminho do cache
    """
    df.to_parquet(cache_path, compression='snappy')
    print(f"Dados salvos em cache: {cache_path}")

def load_cached_data(cache_path):
    """
    Carrega dados do cache se dispon√≠vel
    
    Args:
        cache_path: Caminho do cache
    
    Returns:
        DataFrame ou None se n√£o encontrado
    """
    if os.path.exists(cache_path):
        return pd.read_parquet(cache_path)
    return None
```

## üöÄ Pr√≥ximos Passos

Ap√≥s o processamento dos dados:

1. **Validar** qualidade dos dados processados
2. **Analisar** distribui√ß√µes e estat√≠sticas
3. **Configurar** par√¢metros de treinamento
4. **Executar** tokeniza√ß√£o
5. **Iniciar** fine-tuning

---

**Pr√≥ximo**: [Arquitetura do Sistema](architecture.md)