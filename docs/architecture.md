# ğŸ—ï¸ Arquitetura do Sistema

Este documento explica a arquitetura tÃ©cnica do sistema de fine-tuning PTT5, detalhando cada componente e suas interaÃ§Ãµes.

## ğŸ¯ VisÃ£o Geral da Arquitetura

O sistema implementa uma arquitetura modular baseada em trÃªs pilares principais:

1. **QuantizaÃ§Ã£o 4-bit**: ReduÃ§Ã£o de uso de memÃ³ria
2. **QLoRA**: CombinaÃ§Ã£o de quantizaÃ§Ã£o com adaptadores LoRA
3. **Pipeline de Treinamento**: Processamento completo end-to-end

## ğŸ§© Componentes Principais

### 1. Modelo Base (PTT5)

```python
model_name = "unicamp-dl/ptt5-base-portuguese-vocab"
```

**Por que PTT5?**
- Modelo T5 especÃ­fico para portuguÃªs
- Arquitetura sequence-to-sequence
- PrÃ©-treinado em corpus portuguÃªs
- Tamanho otimizado (220M parÃ¢metros)

### 2. QuantizaÃ§Ã£o 4-bit

```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)
```

**Como funciona:**
- **4-bit**: Cada peso usa apenas 4 bits (vs 32 bits padrÃ£o)
- **Double Quantization**: Quantiza tambÃ©m os fatores de escala
- **NF4**: Tipo de quantizaÃ§Ã£o Neural Float 4
- **ReduÃ§Ã£o**: ~75% menos uso de memÃ³ria

### 3. Adaptadores LoRA

```python
lora_config = LoraConfig(
    r=16,                    # Rank das matrizes
    lora_alpha=32,           # Fator de escala
    target_modules=["q", "v", "k", "o", "wi_0", "wi_1", "wo"],
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)
```

**Conceito LoRA:**
- Congela pesos originais do modelo
- Adiciona matrizes de baixo rank (A e B)
- Treina apenas ~0.1% dos parÃ¢metros
- MantÃ©m qualidade do modelo original

**MatemÃ¡tica:**
```
W_output = W_frozen + (A Ã— B)
```

## ğŸ”„ Pipeline de Processamento

### 1. Carregamento de Dados

```python
df = pd.read_excel(dataset_path)
df = validate_dataframe(df, required_columns)
```

**ValidaÃ§Ãµes aplicadas:**
- VerificaÃ§Ã£o de colunas obrigatÃ³rias
- Limpeza de valores nulos
- NormalizaÃ§Ã£o de texto
- RemoÃ§Ã£o de registros invÃ¡lidos

### 2. PreparaÃ§Ã£o dos Dados

```python
train_data = prepare_data_for_fine_tuning(train_df, prompt_template)
```

**TransformaÃ§Ã£o:**
```
Input: "Gere uma interpretaÃ§Ã£o para a carta 'O Mago' em 'Ano Novo'..."
Output: "2025 Ã© um ano para focar na sua capacidade..."
```

### 3. TokenizaÃ§Ã£o

```python
tokenized_dataset = dataset.map(
    lambda x: tokenize_function(x, tokenizer, max_length=450),
    batched=True
)
```

**Processo:**
- Converte texto em tokens numÃ©ricos
- Aplica padding/truncation
- Cria mÃ¡scaras de atenÃ§Ã£o
- Prepara labels para seq2seq

### 4. Treinamento

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)
```

## ğŸ“Š Fluxo de Dados

```mermaid
graph TD
    A[Dataset Excel] --> B[ValidaÃ§Ã£o]
    B --> C[DivisÃ£o Train/Val]
    C --> D[AplicaÃ§Ã£o Template]
    D --> E[TokenizaÃ§Ã£o]
    E --> F[Data Collator]
    F --> G[Modelo Quantizado]
    G --> H[Adaptadores LoRA]
    H --> I[Treinamento]
    I --> J[AvaliaÃ§Ã£o]
    J --> K[MÃ©tricas ROUGE/BLEU]
    K --> L[Modelo Treinado]
```

## ğŸ›ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### ParÃ¢metros de MemÃ³ria

```yaml
# OtimizaÃ§Ãµes para GPU limitada
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
gradient_checkpointing: true
dataloader_pin_memory: true
```

**CÃ¡lculos:**
- Batch efetivo = batch_size Ã— accumulation_steps
- MemÃ³ria = batch_size Ã— sequence_length Ã— hidden_size
- Checkpointing troca memÃ³ria por computaÃ§Ã£o

### Scheduler de Learning Rate

```yaml
learning_rate: 0.00002
lr_scheduler_type: "cosine"
warmup_ratio: 0.15
```

**Comportamento:**
- Aquecimento: 15% das Ã©pocas
- Decaimento cosseno: Suave reduÃ§Ã£o
- Previne overfitting

## ğŸ” MÃ©tricas de AvaliaÃ§Ã£o

### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

```python
rouge_result = rouge.compute(
    predictions=decoded_preds,
    references=decoded_labels,
    use_stemmer=True
)
```

**Tipos:**
- **ROUGE-1**: SobreposiÃ§Ã£o de unigramas
- **ROUGE-2**: SobreposiÃ§Ã£o de bigramas
- **ROUGE-L**: SubsequÃªncia comum mais longa

### BLEU (Bilingual Evaluation Understudy)

```python
bleu_result = bleu.compute(
    predictions=decoded_preds,
    references=[[label] for label in decoded_labels]
)
```

**CaracterÃ­sticas:**
- MÃ©trica de precisÃ£o
- Penaliza textos muito curtos
- Considera n-gramas atÃ© ordem 4

## ğŸ§  DecodificaÃ§Ã£o Robusta

### Tratamento de Estruturas Complexas

```python
def _handle_complex_structures(data):
    if isinstance(data, (list, tuple)):
        if len(data) > 0 and hasattr(data[0], 'shape'):
            data = data[0]
    return data
```

**Problemas resolvidos:**
- Formatos inconsistentes de prediÃ§Ãµes
- DimensÃµes irregulares
- Tokens invÃ¡lidos
- Estruturas aninhadas

## ğŸ¯ OtimizaÃ§Ãµes Implementadas

### 1. Uso de MemÃ³ria

| TÃ©cnica | ReduÃ§Ã£o | AplicaÃ§Ã£o |
|---------|---------|-----------|
| QuantizaÃ§Ã£o 4-bit | 75% | Pesos do modelo |
| Gradient Checkpointing | 50% | AtivaÃ§Ãµes |
| LoRA | 99% | ParÃ¢metros treinÃ¡veis |

### 2. Velocidade de Treinamento

- **Mixed Precision (FP16)**: 2x mais rÃ¡pido
- **DataLoader optimizado**: ParalelizaÃ§Ã£o
- **Gradient Accumulation**: Batch efetivo maior

### 3. Estabilidade

- **Early Stopping**: Previne overfitting
- **Gradient Clipping**: Previne explosÃ£o
- **Warmup**: Estabiliza inÃ­cio do treinamento

## ğŸ“ Estrutura de Arquivos

```
sistema_le_mat/
â”œâ”€â”€ train_model.py           # Pipeline principal
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_processing.py   # Processamento de dados
â”‚   â””â”€â”€ evaluation_metrics.py # MÃ©tricas robustas
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ pessoa_x_prompt.py   # Templates
â””â”€â”€ config/
    â””â”€â”€ training_config.yaml # ConfiguraÃ§Ãµes
```

## ğŸ”§ Pontos de ExtensÃ£o

### Novos Templates

```python
# Adicionar em pessoa_x_prompt.py
NOVO_TEMPLATE = "Prompt personalizado para {contexto}..."
```

### MÃ©tricas Personalizadas

```python
# Adicionar em evaluation_metrics.py
def custom_metric(predictions, references):
    # Implementar nova mÃ©trica
    pass
```

### ConfiguraÃ§Ãµes EspecÃ­ficas

```yaml
# Adicionar em training_config.yaml
custom_config:
  parameter: value
```

## ğŸš€ PrÃ³ximos Desenvolvimentos

### PossÃ­veis Melhorias

1. **QuantizaÃ§Ã£o 8-bit**: Alternativa Ã  4-bit
2. **Adapter Fusion**: CombinaÃ§Ã£o de mÃºltiplos adapters
3. **Prompt Tuning**: OtimizaÃ§Ã£o de prompts
4. **Multi-GPU**: Treinamento distribuÃ­do

### ExperimentaÃ§Ã£o

```python
# Ãrea para testes
experimental_config = {
    'new_optimizer': 'AdamW',
    'custom_scheduler': 'polynomial',
    'advanced_quantization': True
}
```

---

**PrÃ³ximo**: [ConfiguraÃ§Ã£o do Sistema](configuration.md)