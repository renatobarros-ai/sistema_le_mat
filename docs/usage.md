# üìñ Guia de Uso

Este guia explica como usar o sistema de fine-tuning PTT5, desde a execu√ß√£o b√°sica at√© a interpreta√ß√£o de resultados.

## üöÄ Execu√ß√£o B√°sica

### Comando Principal

```bash
# Executar o treinamento
python train_model.py
```

### Monitoramento em Tempo Real

```bash
# Executar com output detalhado
python train_model.py | tee training_output.log

# Monitorar em outra janela
tail -f results/training_log_*.log
```

## üìä Fases do Treinamento

### 1. Inicializa√ß√£o

```
=== INICIANDO FINE-TUNING PTT5 - SISTEMA DE CARTAS ===
1. Carregando configura√ß√µes de treinamento...
Configura√ß√µes carregadas para o modelo: unicamp-dl/ptt5-base-portuguese-vocab
```

**O que acontece:**
- Carregamento das configura√ß√µes YAML
- Verifica√ß√£o de arquivos e depend√™ncias
- Configura√ß√£o do sistema de logging

### 2. Processamento de Dados

```
2. Carregando e processando dataset...
Dados carregados: 462 registros
Dados ap√≥s valida√ß√£o/limpeza: 462 registros v√°lidos
Cartas √∫nicas: 22
Comprimento m√©dio: 156.7 caracteres
```

**M√©tricas importantes:**
- **Registros v√°lidos**: Quantidade ap√≥s limpeza
- **Cartas √∫nicas**: Diversidade do dataset
- **Comprimento m√©dio**: Complexidade dos textos

### 3. Divis√£o dos Dados

```
Dataset dividido: 392 treino, 70 valida√ß√£o
Datasets criados: 392 treino, 70 valida√ß√£o
```

**Divis√£o estratificada:**
- Mant√©m propor√ß√£o de cartas em treino/valida√ß√£o
- Evita vazamento de dados
- Garante representatividade

### 4. Carregamento do Modelo

```
4. Carregando modelo base e configurando quantiza√ß√£o...
Modelo carregado: unicamp-dl/ptt5-base-portuguese-vocab com quantiza√ß√£o 4-bit
```

**Processo:**
- Download do modelo (primeira vez)
- Aplica√ß√£o da quantiza√ß√£o 4-bit
- Configura√ß√£o dos adaptadores LoRA

### 5. Configura√ß√£o LoRA

```
5. Configurando LoRA...
Par√¢metros trein√°veis configurados
trainable params: 2,359,296 || all params: 225,801,216 || trainable%: 1.0446
```

**Interpreta√ß√£o:**
- Apenas ~1% dos par√¢metros s√£o trein√°veis
- Redu√ß√£o massiva de recursos necess√°rios
- Mant√©m qualidade do modelo base

### 6. Tokeniza√ß√£o

```
6. Tokenizando dados...
Tokeniza√ß√£o conclu√≠da: 392 treino, 70 valida√ß√£o
```

**Transforma√ß√£o:**
- Texto ‚Üí tokens num√©ricos
- Aplica√ß√£o de padding/truncation
- Cria√ß√£o de m√°scaras de aten√ß√£o

### 7. Treinamento

```
10. INICIANDO TREINAMENTO...
VRAM dispon√≠vel: 12.0GB
Batch efetivo: 8
```

**Informa√ß√µes do hardware:**
- VRAM dispon√≠vel para o treinamento
- Batch efetivo calculado
- Otimiza√ß√µes aplicadas

## üìà Interpretando o Progresso

### M√©tricas Durante o Treinamento

```
Epoch 1/20
  Train Loss: 2.1234
  Eval Loss: 1.9876
  Eval ROUGE-L: 0.1234
  Eval BLEU: 0.0987
```

**Interpreta√ß√£o:**
- **Train Loss**: Deve diminuir consistentemente
- **Eval Loss**: Deve diminuir sem overfitting
- **ROUGE-L**: Deve aumentar (0.0-1.0)
- **BLEU**: Deve aumentar (0.0-1.0)

### Progress√£o Saud√°vel

```
Epoch 1: Train Loss: 2.12, Eval Loss: 1.98, ROUGE-L: 0.12
Epoch 5: Train Loss: 1.45, Eval Loss: 1.52, ROUGE-L: 0.25
Epoch 10: Train Loss: 1.02, Eval Loss: 1.15, ROUGE-L: 0.38
Epoch 15: Train Loss: 0.85, Eval Loss: 1.08, ROUGE-L: 0.42
```

**Sinais positivos:**
- Loss de treino diminui suavemente
- Loss de valida√ß√£o segue o treino
- ROUGE-L aumenta consistentemente
- N√£o h√° grande diverg√™ncia entre treino/valida√ß√£o

### Sinais de Alerta

```
# Overfitting
Epoch 10: Train Loss: 0.50, Eval Loss: 1.20, ROUGE-L: 0.15

# Underfitting
Epoch 20: Train Loss: 2.00, Eval Loss: 2.10, ROUGE-L: 0.05

# Instabilidade
Epoch 15: Train Loss: 1.20, Eval Loss: 3.50, ROUGE-L: 0.01
```

## üéØ Resultados Finais

### Relat√≥rio de Conclus√£o

```
=== RESULTADOS FINAIS ===
üìä ROUGE-L Final: 0.4250
üìä BLEU Final: 0.2180
üìä Loss Final: 1.0850
üìÅ M√©tricas detalhadas salvas em: ./results/training_metrics_20250711_143022.json
```

### Interpreta√ß√£o das M√©tricas

| M√©trica | Faixa | Qualidade |
|---------|-------|-----------|
| **ROUGE-L** | 0.0-0.2 | Baixa |
| | 0.2-0.4 | M√©dia |
| | 0.4-0.6 | Boa |
| | 0.6+ | Excelente |
| **BLEU** | 0.0-0.1 | Baixa |
| | 0.1-0.3 | M√©dia |
| | 0.3-0.5 | Boa |
| | 0.5+ | Excelente |

### Salvamento do Modelo

```
13. Salvando modelo treinado...
‚è≥ Salvando adaptadores LoRA... (pode demorar 1-2 minutos)
‚è≥ Salvando tokenizer...
‚úÖ Modelo salvo com sucesso em: ./model_save/lora_model_462_optimized
```

## üìÅ Arquivos Gerados

### Estrutura de Resultados

```
results/
‚îú‚îÄ‚îÄ training_log_20250711_143022.log        # Log completo
‚îú‚îÄ‚îÄ training_metrics_20250711_143022.json   # M√©tricas detalhadas
‚îú‚îÄ‚îÄ checkpoint-120/                         # Checkpoint intermedi√°rio
‚îî‚îÄ‚îÄ checkpoint-240/                         # Checkpoint final
```

### Modelo Salvo

```
model_save/lora_model_462_optimized/
‚îú‚îÄ‚îÄ adapter_config.json                     # Configura√ß√£o LoRA
‚îú‚îÄ‚îÄ adapter_model.bin                       # Pesos dos adaptadores
‚îú‚îÄ‚îÄ tokenizer.json                          # Tokenizer
‚îú‚îÄ‚îÄ tokenizer_config.json                   # Configura√ß√£o do tokenizer
‚îî‚îÄ‚îÄ special_tokens_map.json                 # Tokens especiais
```

## üîç An√°lise Detalhada

### Arquivo de M√©tricas JSON

```json
{
  "timestamp": "20250711_143022",
  "hardware_info": {
    "gpu": "NVIDIA GeForce RTX 4080",
    "vram": "16.0GB",
    "cuda_version": "11.8"
  },
  "dataset_info": {
    "total_samples": 462,
    "train_samples": 392,
    "val_samples": 70,
    "unique_cartas": 22,
    "avg_text_length": 156.7
  },
  "model_config": {
    "model_name": "unicamp-dl/ptt5-base-portuguese-vocab",
    "lora_r": 16,
    "lora_alpha": 32,
    "max_length": 450,
    "epochs_completed": 20
  },
  "training_metrics": {
    "train_runtime": 1234.56,
    "train_samples_per_second": 2.5,
    "total_flos": 1.23e15,
    "train_loss": 0.8543
  },
  "final_eval_metrics": {
    "eval_loss": 1.0850,
    "eval_rougeL": 0.4250,
    "eval_bleu": 0.2180
  }
}
```

## üéÆ Usando o Modelo Treinado

### Carregamento do Modelo

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel

# Carregar modelo base
base_model = AutoModelForSeq2SeqLM.from_pretrained(
    "unicamp-dl/ptt5-base-portuguese-vocab"
)

# Carregar adaptadores LoRA
model = PeftModel.from_pretrained(
    base_model, 
    "./model_save/lora_model_462_optimized"
)

# Carregar tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    "./model_save/lora_model_462_optimized"
)
```

### Gera√ß√£o de Texto

```python
def generate_interpretation(carta, evento, secao, tema):
    prompt = f"Gere uma interpreta√ß√£o para a carta '{carta}' em '{evento}', na se√ß√£o '{secao}', sobre o tema '{tema}', com o estilo da pessoa X."
    
    inputs = tokenizer(prompt, return_tensors="pt", max_length=450, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=450,
            num_beams=4,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.pad_token_id
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Exemplo de uso
resultado = generate_interpretation(
    carta="O Mago",
    evento="Ano Novo",
    secao="Geral",
    tema="Trabalho"
)
print(resultado)
```

## üîß Ajustes Durante o Treinamento

### Early Stopping

```
Early stopping triggered at epoch 12
Best model restored from checkpoint-180
```

**Quando acontece:**
- M√©tricas param de melhorar
- Evita overfitting
- Economiza tempo de treinamento

### Interrup√ß√£o Manual

```bash
# Pressionar Ctrl+C para interromper
^C
‚ö†Ô∏è Treinamento interrompido pelo usu√°rio (Ctrl+C)
üíæ Salvando checkpoint de emerg√™ncia...
‚úÖ Checkpoint de emerg√™ncia salvo em: ./model_save/emergency_checkpoint_20250711_143022
```

## üìä Monitoramento de Recursos

### Uso de GPU

```python
# Monitor simples de GPU
import torch

def monitor_gpu():
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"Mem√≥ria usada: {torch.cuda.memory_used(0)/1024**3:.1f}GB")
        print(f"Mem√≥ria total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
        print(f"Utiliza√ß√£o: {torch.cuda.memory_used(0)/torch.cuda.get_device_properties(0).total_memory*100:.1f}%")

# Executar durante o treinamento
monitor_gpu()
```

### Tempo de Treinamento

```
Tempo estimado por √©poca: 2-3 minutos
Tempo total estimado: 40-60 minutos
```

**Fatores que afetam o tempo:**
- Tamanho do dataset
- Poder da GPU
- Configura√ß√µes de batch
- Comprimento das sequ√™ncias

## üéØ Pr√≥ximos Passos

Ap√≥s o treinamento bem-sucedido:

1. **Avaliar** resultados nas m√©tricas
2. **Testar** o modelo com exemplos
3. **Ajustar** configura√ß√µes se necess√°rio
4. **Documentar** os experimentos
5. **Implementar** em produ√ß√£o

---

**Pr√≥ximo**: [Troubleshooting](troubleshooting.md)