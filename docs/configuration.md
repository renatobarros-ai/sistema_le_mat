# ConfiguraÃ§Ã£o do Sistema

Este guia explica como configurar e personalizar o sistema de fine-tuning para suas necessidades especÃ­ficas.

## ðŸ“‹ Arquivo de ConfiguraÃ§Ã£o Principal

O arquivo `config/training_config.yaml` contÃ©m todas as configuraÃ§Ãµes do sistema:

```yaml
# Estrutura principal
model_config:      # ConfiguraÃ§Ãµes do modelo
lora_config:       # ParÃ¢metros LoRA
training_args:     # Argumentos de treinamento
early_stopping:    # Parada antecipada
dataset_path:      # Caminho dos dados
test_size:         # Tamanho do conjunto de validaÃ§Ã£o
random_state:      # Seed para reprodutibilidade
```

## ConfiguraÃ§Ãµes do Modelo

### Modelo Base

```yaml
model_config:
  name: "unicamp-dl/ptt5-base-portuguese-vocab"
  max_length: 450
```

**Modelo utilizado:**
- `ptt5-base`: Modelo base portuguÃªs otimizado (configurado no YAML)

### QuantizaÃ§Ã£o 4-bit

```yaml
quantization:
  load_in_4bit: true
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "torch.float16"
```

**ParÃ¢metros:**
- `load_in_4bit`: Habilita quantizaÃ§Ã£o (sempre true)
- `bnb_4bit_use_double_quant`: Dupla quantizaÃ§Ã£o (recomendado)
- `bnb_4bit_quant_type`: Tipo de quantizaÃ§Ã£o (`nf4` ou `fp4`)
- `bnb_4bit_compute_dtype`: Tipo de dados (`float16` ou `bfloat16`)

## ConfiguraÃ§Ãµes LoRA

### ParÃ¢metros Principais

```yaml
lora_config:
  r: 16                    # Rank das matrizes
  lora_alpha: 32           # Fator de escala
  lora_dropout: 0.1        # Dropout para regularizaÃ§Ã£o
  bias: "none"             # ConfiguraÃ§Ã£o de bias
  task_type: "SEQ_2_SEQ_LM"
```

### MÃ³dulos Alvo

```yaml
target_modules: ["q", "v", "k", "o", "wi_0", "wi_1", "wo"]
```

**ExplicaÃ§Ã£o dos mÃ³dulos:**
- `q`, `k`, `v`: Matrizes de atenÃ§Ã£o (Query, Key, Value)
- `o`: ProjeÃ§Ã£o de saÃ­da da atenÃ§Ã£o
- `wi_0`, `wi_1`: Camadas feed-forward
- `wo`: SaÃ­da do feed-forward

### Ajustes por CenÃ¡rio

| CenÃ¡rio | r | alpha | dropout | Motivo |
|---------|---|-------|---------|---------|
| **Dados poucos** | 8 | 16 | 0.1 | Previne overfitting |
| **Dados mÃ©dios** | 16 | 32 | 0.1 | ConfiguraÃ§Ã£o padrÃ£o |
| **Dados muitos** | 32 | 64 | 0.05 | Maior capacidade |

## ConfiguraÃ§Ãµes de Treinamento

### Batch Size e AcumulaÃ§Ã£o

```yaml
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
```

**CÃ¡lculo do batch efetivo:**
```
Batch efetivo = batch_size Ã— accumulation_steps Ã— num_gpus
```

**Ajustes por GPU:**

| GPU | VRAM | Batch Size | Accumulation |
|-----|------|------------|-------------|
| RTX 3060 | 12GB | 1 | 8 |
| RTX 3070 | 8GB | 1 | 4 |
| RTX 4080 | 16GB | 2 | 8 |
| RTX 4090 | 24GB | 4 | 8 |

### Learning Rate

```yaml
learning_rate: 0.00002
lr_scheduler_type: "cosine"
warmup_ratio: 0.15
```

**Valores recomendados:**
- **Pequeno dataset**: 0.00001 (mais conservador)
- **MÃ©dio dataset**: 0.00002 (padrÃ£o)
- **Grande dataset**: 0.00003 (mais agressivo)

### Ã‰pocas e Salvamento

```yaml
num_train_epochs: 20
save_steps: 60
eval_strategy: "epoch"
save_strategy: "epoch"
```

**EstratÃ©gias:**
- `"epoch"`: Avalia/salva a cada Ã©poca
- `"steps"`: Avalia/salva a cada N steps
- `"no"`: NÃ£o avalia durante o treinamento

## ConfiguraÃ§Ãµes de AvaliaÃ§Ã£o

### MÃ©tricas

```yaml
metric_for_best_model: "eval_rougeL"
greater_is_better: true
load_best_model_at_end: true
```

**MÃ©tricas disponÃ­veis:**
- `eval_rougeL`: ROUGE-L (recomendado)
- `eval_bleu`: BLEU
- `eval_loss`: Perda de validaÃ§Ã£o

### Early Stopping

```yaml
early_stopping:
  patience: 4
  threshold: 0.001
```

**ConfiguraÃ§Ã£o:**
- `patience`: Ã‰pocas sem melhoria antes de parar
- `threshold`: Melhoria mÃ­nima considerada

## ConfiguraÃ§Ãµes de Dados

### Dataset

```yaml
dataset_path: "./database/db_462.xlsx"
test_size: 0.15
random_state: 42
```

**ParÃ¢metros:**
- `dataset_path`: Caminho para o arquivo Excel
- `test_size`: ProporÃ§Ã£o para validaÃ§Ã£o (0.1 = 10%)
- `random_state`: Seed para reprodutibilidade

### Formato dos Dados

O Excel deve conter estas colunas:

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `evento` | string | Contexto do evento |
| `carta` | string | Nome da carta |
| `tema` | string | Tema da interpretaÃ§Ã£o |
| `secao` | string | SeÃ§Ã£o especÃ­fica |
| `texto` | string | Texto de saÃ­da esperado |

## ConfiguraÃ§Ãµes de Prompts

### Template Principal

```python
# Em prompts/pessoa_x_prompt.py
PESSOA_X_FINE_TUNING_PROMPT = (
    "Gere uma interpretaÃ§Ã£o para a carta '{carta}' "
    "em '{evento}', na seÃ§Ã£o '{secao}', "
    "sobre o tema '{tema}', com o estilo da pessoa X."
)
```

### PersonalizaÃ§Ãµes

```python
# Exemplo de template personalizado
TEMPLATE_FORMAL = (
    "ForneÃ§a uma anÃ¡lise detalhada da carta '{carta}' "
    "no contexto de '{evento}', seÃ§Ã£o '{secao}', "
    "focando em '{tema}'."
)

TEMPLATE_CASUAL = (
    "Que tal uma interpretaÃ§Ã£o da carta '{carta}' "
    "para '{evento}' em '{secao}', "
    "pensando em '{tema}'?"
)
```

## ConfiguraÃ§Ãµes AvanÃ§adas

### OtimizaÃ§Ãµes de MemÃ³ria

```yaml
# Para GPUs com pouca memÃ³ria
gradient_checkpointing: true
dataloader_pin_memory: true
max_grad_norm: 1.0
```

### OtimizaÃ§Ãµes de Velocidade

```yaml
# Para treinamento mais rÃ¡pido
fp16: true
dataloader_num_workers: 2
remove_unused_columns: false
```

### ConfiguraÃ§Ãµes de Logging

```yaml
logging_steps: 8
report_to: "none"
```

## ConfiguraÃ§Ãµes por CenÃ¡rio

### CenÃ¡rio 1: GPU Limitada (4-6GB)

```yaml
model_config:
  max_length: 256
training_args:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  fp16: true
```

### CenÃ¡rio 2: GPU MÃ©dia (8-12GB)

```yaml
model_config:
  max_length: 450
training_args:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  fp16: true
```

### CenÃ¡rio 3: GPU Potente (16GB+)

```yaml
model_config:
  max_length: 512
training_args:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  gradient_checkpointing: false
  fp16: true
```

## ConfiguraÃ§Ãµes Experimentais

### Testes A/B

```yaml
# Arquivo: config/experimental_config.yaml
experimental:
  different_optimizer: "adamw_hf"
  custom_scheduler: "polynomial"
  advanced_quantization:
    bnb_4bit_quant_type: "fp4"
```

### ConfiguraÃ§Ãµes Debug

```yaml
# Para debugging
training_args:
  max_steps: 100
  eval_steps: 10
  save_steps: 10
  logging_steps: 1
```

## Ferramentas de ConfiguraÃ§Ã£o

### ValidaÃ§Ã£o de ConfiguraÃ§Ã£o

```python
# Script para validar configuraÃ§Ãµes
def validate_config(config_path):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # ValidaÃ§Ãµes
    assert config['model_config']['max_length'] > 0
    assert config['training_args']['learning_rate'] > 0
    assert config['test_size'] < 1.0
    
    print("âœ… ConfiguraÃ§Ã£o vÃ¡lida!")
```

### Calculadora de Batch

```python
def calculate_effective_batch(device_batch, accumulation, num_gpus=1):
    return device_batch * accumulation * num_gpus

# Exemplo
effective_batch = calculate_effective_batch(1, 8, 1)
print(f"Batch efetivo: {effective_batch}")
```

## ðŸ“ Backup e Versionamento

### Backup de ConfiguraÃ§Ãµes

```bash
# Criar backup antes de alteraÃ§Ãµes
cp config/training_config.yaml config/training_config_backup.yaml
```

### ConfiguraÃ§Ãµes Versionadas

```
config/
â”œâ”€â”€ training_config.yaml        # Atual
â”œâ”€â”€ training_config_v1.yaml     # VersÃ£o 1
â”œâ”€â”€ training_config_v2.yaml     # VersÃ£o 2
â””â”€â”€ experimental_config.yaml    # Experimentos
```

## PrÃ³ximos Passos

ApÃ³s configurar o sistema:

1. **Validar** as configuraÃ§Ãµes
2. **Testar** com dataset pequeno
3. **Monitorar** o treinamento
4. **Ajustar** conforme necessÃ¡rio
5. **Documentar** as alteraÃ§Ãµes

---

**PrÃ³ximo**: [Guia de Uso](usage.md)